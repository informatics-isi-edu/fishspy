#!/usr/bin/python3

import json
import pika
from deriva.core import PollingErmrestCatalog, HatracStore, urlquote, get_credential, DEFAULT_CREDENTIAL_FILE
import os
import re
import sys
import platform
import time
import json
import datetime
import pytz
import atexit
import shutil
import tempfile
import subprocess

# server to talk to... defaults to our own FQDN
servername = os.getenv('FISHSPY_SERVER', platform.uname()[1])

scriptdir = os.getenv('FISHSPY_PATH', '')
scriptdir = '%s/' % scriptdir if scriptdir else ''

credentials = get_credential(
    servername,
    credential_file=os.getenv('FISHSPY_CREDENTIALS', DEFAULT_CREDENTIAL_FILE)
)

# handle to /dev/null we can use in Popen() calls below...
fnull = open(os.devnull, 'r+b')

# we want a temporary work space for our external processing scripts
startup_working_dir = os.getcwd()
working_dir = None

class WorkerRuntimeError (RuntimeError):
    pass

class WorkerNotReadyError (RuntimeError):
    pass

@atexit.register
def cleanup():
    """Delete temporary working dir and its contents.

       We want to run this on any exit including exception/crashes so
       we register it with runtime.

       We also call it during each major work cycle to reap space and
       then create a new temporary work area again.
    """
    global working_dir
    if working_dir is not None:
        sys.stderr.write('Purging working directory %s... ' % working_dir)
        os.chdir(startup_working_dir)
        shutil.rmtree(working_dir)
        sys.stderr.write('done.\n')
        working_dir = None

# these are peristent/logical connections so we create once and reuse
# they can retain state and manage an actual HTTP connection-pool
catalog = PollingErmrestCatalog(
    'https', 
    servername,
    '1',
    credentials
)

store = HatracStore(
    'https', 
    servername,
    credentials
)

### Custom work to run on job events...
def analyze_movie(row, debug_video=False):
    """Process input row and return (events, frames, debug) filenames.

       Arguments:
         row: the Zebrafish.Behavior record
         debug_video: whether to produce debug M4V

       Result: (events, frames, debug)
         events: event CSV relative filename
         frames: frame measures CSV relative filename
         debug:  debug M4V relative filename or None
    """
    raw_url = row['Raw URL']
    movie_id = 'Behavior_%s' % row['RID']
    raw_filename = '%s.m4v' % movie_id

    # download the content to temp dir
    try:
        store.get_obj(raw_url, destfilename=raw_filename)
    except Exception as e:
        raise WorkerRuntimeError('could not download raw movie: %s' % e)

    # external analysis script arguments
    command = [
        scriptdir + 'fishspy-analyze-movie',
        raw_filename
    ]

    # build up environment for external analysis script
    env = { 'FISHSPY_LOGLEVEL': os.getenv('FISHSPY_LOGLEVEL', 'error') }
    if debug_video:
        env['DEBUG_MOVIE'] = 'true'

    # allow supported parameters to be set per instrument
    r = catalog.get(
        '/entity/Zebrafish:Behavior/RID=%s/Synapse:FScope' % urlquote(row['RID']),
    )
    r.raise_for_status()
    analysis_params = r.json()[0]['Analysis Parameters']
    sys.stderr.write('Got scope parameters: %r\n' % analysis_params)
    if type(analysis_params) is dict:
        for k, v in analysis_params.items():
            if k in {'FISHSPY_CROP', 'FISHSPY_CONTRAST', 'FISHSPY_GAMMA', 'DEBUG_MOVIE'}:
                env[k] = '%s' % v
            else:
                raise WorkerRuntimeError('Unsupported analysis parameter %r=%r' % (k, v))
    elif analysis_params is not None:
        raise WorkerRuntimeError('Analysis parameters %r are not an object!' % analysis_params)

    debug_video = env.get('DEBUG_MOVIE', 'false').lower() == 'true'

    # run it to completion
    sys.stderr.write('Using processing environment %r\n' % (env,))
    analysis = subprocess.Popen(command, stdin=fnull, env=env)
    code = analysis.wait()
    del analysis
    if code != 0:
        raise WorkerRuntimeError('Non-zero analysis exit status %s!' % code)

    return (
        '%s.events.csv' % movie_id,
        '%s.frame_measures.csv' % movie_id,
        ('%s.debug.m4v' % movie_id) if debug_video else None
    )

def plot_movie_local(row, events_filename, frames_filename):
    """Process analysis data and return output filenames.
    
       Arguments:
         row: the experiment record from ERMrest
         events_filename: local file w/ events CSV
         frames_filename: local file w/ frame-measures CSV

       Result: local filenames for (plot PNG, plot3 PNG, sums CSV, grade JSON)

    """
    movie_id = 'Behavior_%s' % row['RID']
    assert events_filename == ('%s.events.csv' % movie_id)
    assert frames_filename == ('%s.frame_measures.csv' % movie_id)

    command = [
        scriptdir + 'fishspy-plot-results',
        events_filename,
        frames_filename
    ]

    env = { 'FISHSPY_TRIAL_COUNTS': row['Trial Counts'] } if row['Trial Counts'] else {}

    plot = subprocess.Popen(command, stdin=fnull, env=env)
    code = plot.wait()
    del plot
    if code != 0:
        raise WorkerRuntimeError('Non-zero plot exit status %s!' % code)

    return (
        '%s.plot3.png' % movie_id,
        '%s.sums.csv' % movie_id,
        '%s.grade.json' % movie_id,
    )

def plot_movie(row):
    """Plot movie data for experiment record.

       Arguments:
         row: record from ERMrest

       Result: local filenames for (plot PNG, sums CSV)

    """
    movie_id = 'Behavior_%s' % row['RID']

    events_filename = '%s.events.csv' % movie_id
    frames_filename = '%s.frame_measures.csv' % movie_id

    store.get_obj(row['Events URL'], destfilename=events_filename)
    store.get_obj(row['Frames URL'], destfilename=frames_filename)

    return plot_movie_local(row, events_filename, frames_filename)

def run_row_job(row):
    """Do idempotent fishspy analysis on experiment record.

       Arguments:
         row: experiment record from ERMrest

       Results: None

       Performs idempotent update of row in ERMrest if possible,
       also creates objects in Hatrac as needed.

    """
    global working_dir
    
    sys.stderr.write('Claimed job %s.\n' % row['RID'])

    working_dir = tempfile.mkdtemp(dir="/var/tmp")
    os.chdir(working_dir)
    sys.stderr.write('Using working directory %s.\n' % working_dir)

    try:
        if not row['Raw URL']:
            raise WorkerRuntimeError('empty Raw URL')

        # where we put stuff in hatrac
        subject_path = '/hatrac/Zf/Zf_%s' % row['Subject']

        if row['Events URL'] is None or row['Frames URL'] is None:
            # download and analyze raw movie
            analyze_outputs = analyze_movie(row)

            # upload analysis results
            events_obj = '%s/Behavior_%s.events.csv' % (subject_path, row['RID'])
            events_url = store.put_loc(events_obj, analyze_outputs[0], headers={'Content-Type': 'text/csv'})

            frames_obj = '%s/Behavior_%s.frame_measures.csv' % (subject_path, row['RID'])
            frames_url = store.put_loc(frames_obj, analyze_outputs[1], headers={'Content-Type': 'text/csv'})

            updated_row = {
                "RID": row["RID"],
                "Status": "processed",
                "Events URL": events_url,
                "Frames URL": frames_url,
            }
            
            if analyze_outputs[2]:
                # may or may not have generated debug movie?
                debug_obj = '%s/Behavior_%s.debug.m4v' % (subject_path, row['RID'])
                debug_url = store.put_loc(debug_obj, analyze_outputs[2], headers={'Content-Type': 'video/x-m4v'})

                updated_row.update({
                    "Debug URL": debug_url
                })

            # record analysis results in ermrest
            catalog.put(
                '/attributegroup/Zebrafish:Behavior/RID;%s' % ','.join([
                    urlquote(col, safe='')
                    for col in updated_row.keys()
                    if col != 'RID'
                ]),
                json=[updated_row]
            )
            sys.stderr.write('\nupdated in ERMrest: %s' % json.dumps(updated_row, indent=2))
            
        else:
            # skip straight to plotting remote analysis object
            analyze_outputs = None

        sys.stderr.write('Analysis complete.\n')

        if analyze_outputs is not None:
            # plot local products
            plot_outputs = plot_movie_local(row, *analyze_outputs[0:2])
        else:
            # plot existing analysis data from object store
            plot_outputs = plot_movie(
                row
            )

        # upload plot outputs
        updated_row = {
            "RID": row["RID"],
            "Status": "processed",
        }
        obj_prefix = '%s/Behavior_%s' % (subject_path, row['RID'])
        for idx, obj_suffix, mtype, cname in [
                (0, 'plot3.png', 'image/png', 'Plot URL'),
                (1, 'sums.csv', 'text/csv', 'Sums URL'),
        ]:
            if row[cname] is None:
                updated_row[cname] = store.put_loc(
                    '%s.%s' % (obj_prefix, obj_suffix),
                    plot_outputs[idx],
                    headers={'Content-Type': mtype}
                )

        with open(plot_outputs[2], 'r') as inf:
            v = json.load(inf)
            summary = v["summary"]
            term = 'SYNAPSE:%s' % summary
            if updated_row.get('Grade Details') != v:
                updated_row['Grade Details'] = v
            if updated_row.get('Grade') != term:
                updated_row['Grade'] = term

        # record plot result in ermrest
        catalog.put(
            '/attributegroup/Zebrafish:Behavior/RID;%s' % ','.join([
                urlquote(col, safe='')
                for col in updated_row.keys()
                if col != 'RID'
            ]),
            json=[updated_row]
        )
        sys.stderr.write('\nupdated in ERMrest: %s' % json.dumps(updated_row, indent=2))
        
    finally:
        sys.stderr.write('\n')
        cleanup()

# for state-tracking across look_for_work() iterations
idle_etag = None

def look_for_work():
    """Find, claim, and process experiment one record.

       1. Find row with actionable state (partial data and Status=None
       2. Claim by setting Status="in progress"
       3. Download and process data as necessary
       4. Upload processing results to Hatrac
       5. Update ERMrest w/ processing result URLs and Status="processed"

       Do find/claim with HTTP opportunistic concurrency control and
       caching for efficient polling and quiescencs.

       On error, set Status="failed: reason"

       Result:
         true: there might be more work to claim
         false: we failed to find any work

    """
    global idle_etag

    claimable_work_url = '/entity/Zebrafish:Behavior/!Raw%20URL::null::/!FScope::null::/Events%20URL::null::;Frames%20URL::null::;Plot%20URL::null::;Sums%20URL::null::;Grade::null::/Status::null::;Status=null?limit=1'
    status_update_url = '/attributegroup/Zebrafish:Behavior/RID;Status'
    
    # this handled concurrent update for us to safely and efficiently claim a record
    idle_etag, batch = catalog.state_change_once(
        claimable_work_url,
        status_update_url,
        lambda row: {'RID': row['RID'], 'Status': "in progress"},
        idle_etag
    )

    # we used a batch size of 1 due to ?limit=1 above...
    for row, claim in batch:
        try:
            run_row_job(row)
        except Exception as e:
            # TODO: eat some exceptions and return True to continue?
            catalog.put(status_update_url, json=[{'RID': row['RID'], 'Status': "failed: %s" % e }])
            raise

        return True
    else:
        return False
        
catalog.blocking_poll(look_for_work)

