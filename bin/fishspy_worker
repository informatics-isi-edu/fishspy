#!/usr/bin/python

import json
import pika
from deriva_common import PollingErmrestCatalog, HatracStore
import os
import re
import sys
import platform
import time
import json
import datetime
import pytz
import atexit
import shutil
import tempfile
import subprocess
import urllib

# server to talk to... defaults to our own FQDN
servername = os.getenv('FISHSPY_SERVER', platform.uname()[1])

# secret session cookie
credfile = os.getenv('FISHSPY_CREDENTIALS', 'credentials.json')
credentials = json.load(open(credfile))

# handle to /dev/null we can use in Popen() calls below...
fnull = open(os.devnull, 'r+b')

# we want a temporary work space for our external processing scripts
startup_working_dir = os.getcwd()
working_dir = None

class WorkerRuntimeError (RuntimeError):
    pass

class WorkerNotReadyError (RuntimeError):
    pass

@atexit.register
def cleanup():
    """Delete temporary working dir and its contents.

       We want to run this on any exit including exception/crashes so
       we register it with runtime.

       We also call it during each major work cycle to reap space and
       then create a new temporary work area again.
    """
    global working_dir
    if working_dir is not None:
        sys.stderr.write('Purging working directory %s... ' % working_dir)
        os.chdir(startup_working_dir)
        shutil.rmtree(working_dir)
        sys.stderr.write('done.\n')
        working_dir = None

# these are peristent/logical connections so we create once and reuse
# they can retain state and manage an actual HTTP connection-pool
catalog = PollingErmrestCatalog(
    'https', 
    servername,
    '1',
    credentials
)

store = HatracStore(
    'https', 
    servername,
    credentials
)

### Custom work to run on job events...
def analyze_movie(row, debug_video=False):
    """Process input row and return (events, frames, debug) filenames.

       Arguments:
         row: the Zebrafish.Behavior record
         debug_video: whether to produce debug M4V

       Result: (events, frames, debug)
         events: event CSV relative filename
         frames: frame measures CSV relative filename
         debug:  debug M4V relative filename or None
    """
    raw_url = row['Raw URL']
    movie_id = row['ID']
    raw_filename = '%s.m4v' % movie_id

    # download the content to temp dir
    store.get_obj(raw_url, destfilename=raw_filename)

    # external analysis script arguments
    command = [
        'fishspy-analyze-movie',
        raw_filename
    ]

    # build up environment for external analysis script
    env = { 'FISHSPY_LOGLEVEL': os.getenv('FISHSPY_LOGLEVEL', 'error') }
    if debug_video:
        env['DEBUG_MOVIE'] = 'true'

    # allow supported parameters to be set per instrument
    r = catalog.get(
        '/entity/Synapse:FScope/ID=%s' % urllib.quote(row['FScope']),
    )
    r.raise_for_status()
    analysis_params = r.json()[0]['Analysis Parameters']
    sys.stderr.write('Got scope parameters: %r\n' % analysis_params)
    if type(analysis_params) is dict:
        for k, v in analysis_params.items():
            if k in {'FISHSPY_CROP', 'FISHSPY_CONTRAST', 'FISHSPY_GAMMA'}:
                env[k] = '%s' % v
            else:
                raise WorkerRuntimeError('Unsupported analysis parameter %r=%r' % (k, v))
    elif analysis_params is not None:
        raise WorkerRuntimeError('Analysis parameters %r are not an object!' % analysis_params)

    # run it to completion
    sys.stderr.write('Using processing environment %r\n' % (env,))
    analysis = subprocess.Popen(command, stdin=fnull, env=env)
    code = analysis.wait()
    del analysis
    if code != 0:
        raise WorkerRuntimeError('Non-zero analysis exit status %s!' % code)

    return (
        '%s.events.csv' % movie_id,
        '%s.frame_measures.csv' % movie_id,
        ('%s.debug.m4v' % movie_id) if debug_video else None
    )

def plot_movie_local(row, events_filename, frames_filename):
    """Process analysis data and return plot filename.
    
       Arguments:
         row: the experiment record from ERMrest
         events_filename: local file w/ events CSV
         frames_filename: local file w/ frame-measures CSV

       Result: local filename for plot PNG

    """
    assert events_filename == ('%s.events.csv' % row['ID'])
    assert frames_filename == ('%s.frame_measures.csv' % row['ID'])

    command = [
        'fishspy-plot-results',
        events_filename,
        frames_filename
    ]

    env = { 'FISHSPY_TRIAL_COUNTS': row['Trial Counts'] } if row['Trial Counts'] else {}

    plot = subprocess.Popen(command, stdin=fnull, env=env)
    code = plot.wait()
    del plot
    if code != 0:
        raise WorkerRuntimeError('Non-zero plot exit status %s!' % code)

    return '%s.plot.png' % row['ID']

def plot_movie(row):
    """Plot movie data for experiment record.

       Arguments:
         row: record from ERMrest

       Result: local filename for plot PNG based on row.

    """
    movie_id = row['ID']

    events_filename = '%s.events.csv' % movie_id
    frames_filename = '%s.frame_measures.csv' % movie_id

    store.get_obj(row['Events URL'], destfilename=events_filename)
    store.get_obj(row['Frames URL'], destfilename=frames_filename)

    return plot_movie_local(row, events_filename, frames_filename)

def run_row_job(row):
    """Do idempotent fishspy analysis on experiment record.

       Arguments:
         row: experiment record from ERMrest

       Results: None

       Performs idempotent update of row in ERMrest if possible,
       also creates objects in Hatrac as needed.

    """
    global working_dir
    
    sys.stderr.write('Claimed job %s.\n' % row['ID'])

    working_dir = tempfile.mkdtemp(dir="/var/tmp")
    os.chdir(working_dir)
    sys.stderr.write('Using working directory %s.\n' % working_dir)

    try:
        if not row['Raw URL']:
            raise WorkerRuntimeError('empty Raw URL')

        # where we put stuff in hatrac
        subject_path = '/hatrac/Zf/%s' % row['Subject']

        if row['Events URL'] is None or row['Frames URL'] is None:
            # download and analyze raw movie
            analyze_outputs = analyze_movie(row)

            # upload analysis results
            events_obj = '%s/%s.events.csv' % (subject_path, row['ID'])
            events_url = store.put_loc(events_obj, analyze_outputs[0], headers={'Content-Type': 'text/csv'})

            frames_obj = '%s/%s.frame_measures.csv' % (subject_path, row['ID'])
            frames_url = store.put_loc(frames_obj, analyze_outputs[1], headers={'Content-Type': 'text/csv'})

            updated_row = {
                "ID": row["ID"],
                "Status": "processed",
                "Events URL": events_url,
                "Frames URL": frames_url,
                "Events File Type": "csv",
                "Frames File Type": "csv"
            }
            
            if analyze_outputs[2]:
                # may or may not have generated debug movie?
                debug_obj = '%s/%s.debug.m4v' % (subject_path, row['ID'])
                debug_url = store.put_loc(debug_obj, analyze_outputs[2], headers={'Content-Type': 'video/x-m4v'})

                updated_row.update({
                    "Debug URL": debug_url,
                    "Debug File Type": "m4v"
                })

            # record analysis results in ermrest
            catalog.put(
                '/attributegroup/Zebrafish:Behavior/ID;%s' % ','.join([
                    urllib.quote(col, safe='')
                    for col in updated_row.keys()
                    if col != 'ID'
                ]),
                json=[updated_row]
            )
            sys.stderr.write('\nupdated in ERMrest: %s' % json.dumps(updated_row, indent=2))
            
        else:
            # skip straight to plotting remote analysis object
            analyze_outputs = None

        sys.stderr.write('Analysis complete.\n')

        if analyze_outputs is not None:
            # plot local products
            plot_output = plot_movie_local(row, *analyze_outputs[0:2])
        else:
            # plot existing analysis data from object store
            plot_output = plot_movie(
                row
            )

        # upload plot
        plot_obj = '%s/%s.plot.png' % (subject_path, row['ID'])
        plot_url = store.put_loc(plot_obj, plot_output, headers={'Content-Type': 'image/png'})

        updated_row = {
            "ID": row["ID"],
            "Status": "processed",
            "Plot URL": plot_url,
            "Plot File Type": "png"
        }

        # record plot result in ermrest
        catalog.put(
            '/attributegroup/Zebrafish:Behavior/ID;%s' % ','.join([
                urllib.quote(col, safe='')
                for col in updated_row.keys()
                if col != 'ID'
            ]),
            json=[updated_row]
        )
        sys.stderr.write('\nupdated in ERMrest: %s' % json.dumps(updated_row, indent=2))
        
    finally:
        sys.stderr.write('\n')
        cleanup()

# for state-tracking across look_for_work() iterations
idle_etag = None

def look_for_work():
    """Find, claim, and process experiment one record.

       1. Find row with actionable state (partial data and Status=None
       2. Claim by setting Status="in progress"
       3. Download and process data as necessary
       4. Upload processing results to Hatrac
       5. Update ERMrest w/ processing result URLs and Status="processed"

       Do find/claim with HTTP opportunistic concurrency control and
       caching for efficient polling and quiescencs.

       On error, set Status="failed: reason"

       Result:
         true: there might be more work to claim
         false: we failed to find any work

    """
    global idle_etag

    claimable_work_url = '/entity/Zebrafish:Behavior/!Raw%20URL::null::/Events%20URL::null::;Frames%20URL::null::;Plot%20URL::null::/Status::null::;Status=null?limit=1'
    status_update_url = '/attributegroup/Zebrafish:Behavior/ID;Status'
    
    # this handled concurrent update for us to safely and efficiently claim a record
    idle_etag, batch = catalog.state_change_once(
        claimable_work_url,
        status_update_url,
        lambda row: {'ID': row['ID'], 'Status': "in progress"},
        idle_etag
    )

    # we used a batch size of 1 due to ?limit=1 above...
    for row, claim in batch:
        try:
            run_row_job(row)
        except Exception, e:
            # TODO: eat some exceptions and return True to continue?
            catalog.put(status_update_url, json=[{'ID': row['ID'], 'Status': "failed: %s" % e }])
            raise

        return True
    else:
        return False
        
catalog.blocking_poll(look_for_work)

